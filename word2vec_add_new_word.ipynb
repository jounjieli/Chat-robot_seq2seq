{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from ML_utils import ML_utils\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將PTT的語料進行斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\jieba_dict\\dict.txt.big ...\n",
      "2019-03-25 13:52:10,238 : DEBUG : Building prefix dict from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\jieba_dict\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u7f3fa752c709ba36f0bb44660655141f.cache\n",
      "2019-03-25 13:52:10,243 : DEBUG : Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u7f3fa752c709ba36f0bb44660655141f.cache\n",
      "Loading model cost 1.007 seconds.\n",
      "2019-03-25 13:52:11,248 : DEBUG : Loading model cost 1.007 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-03-25 13:52:11,249 : DEBUG : Prefix dict has been built succesfully.\n",
      "2019-03-25 13:52:11,918 : INFO : 已完成前 10000 行的斷詞\n",
      "2019-03-25 13:52:12,596 : INFO : 已完成前 20000 行的斷詞\n",
      "2019-03-25 13:52:13,247 : INFO : 已完成前 30000 行的斷詞\n",
      "2019-03-25 13:52:13,914 : INFO : 已完成前 40000 行的斷詞\n",
      "2019-03-25 13:52:14,577 : INFO : 已完成前 50000 行的斷詞\n",
      "2019-03-25 13:52:15,237 : INFO : 已完成前 60000 行的斷詞\n",
      "2019-03-25 13:52:15,913 : INFO : 已完成前 70000 行的斷詞\n",
      "2019-03-25 13:52:16,585 : INFO : 已完成前 80000 行的斷詞\n",
      "2019-03-25 13:52:17,240 : INFO : 已完成前 90000 行的斷詞\n",
      "2019-03-25 13:52:17,978 : INFO : 已完成前 100000 行的斷詞\n",
      "2019-03-25 13:52:18,740 : INFO : 已完成前 110000 行的斷詞\n"
     ]
    }
   ],
   "source": [
    "load_userdict_path = './jieba_dict/ptt_userdict.txt'\n",
    "dir_path = r\"./word2vec_model\"\n",
    "ML_utils.CallF_DirFile_save(dir_path, ML_utils.Jieba_file_segmentation, replace_old=False,regular=True,\n",
    "                            file_filter_='^ptt_data',load_userdict_path=load_userdict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "句子前後都已經加上 < EOS >、 < SOS > 的token，將用wiki訓練的model使用這次要訓練的資料，訓練新的字詞。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-23 16:11:54,904 : INFO : loading Word2Vec object from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model\n",
      "2019-03-23 16:11:56,383 : INFO : loading wv recursively from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model.wv.* with mmap=None\n",
      "2019-03-23 16:11:56,384 : INFO : loading vectors from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model.wv.vectors.npy with mmap=None\n",
      "2019-03-23 16:11:56,738 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-03-23 16:11:56,739 : INFO : loading vocabulary recursively from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model.vocabulary.* with mmap=None\n",
      "2019-03-23 16:11:56,740 : INFO : loading trainables recursively from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model.trainables.* with mmap=None\n",
      "2019-03-23 16:11:56,741 : INFO : loading syn1neg from D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model.trainables.syn1neg.npy with mmap=None\n",
      "2019-03-23 16:11:57,090 : INFO : setting ignored attribute cum_table to None\n",
      "2019-03-23 16:11:57,091 : INFO : loaded D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_modle_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model\"\n",
    "model = word2vec.Word2Vec.load(word2vec_modle_path)\n",
    "n_word = print(len(model.wv.index2word))\n",
    "try:\n",
    "    model.wv.most_similar(positive='<SOS>',topn=1)\n",
    "except KeyError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    model.wv.most_similar(positive='<EOS>',topn=1)\n",
    "except KeyError as err:\n",
    "    print(err)    \n",
    "model.vocabulary.min_count = 5\n",
    "file_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\ptt_train_vec.txt\"\n",
    "sentences = word2vec.LineSentence(file_path)\n",
    "model.build_vocab(sentences, update=True)#训练该行\n",
    "model.train(sentences,total_examples= model.corpus_count,epochs= model.epochs)\n",
    "print(len(model.wv.index2word))\n",
    "print(model.wv.most_similar(positive='<SOS>',topn=5))\n",
    "print(model.wv.most_similar(positive='<EOS>',topn=5))\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\word2vec_model\"\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('魔獸', 0.4385209083557129),\n",
       " ('TERRA', 0.4252971112728119),\n",
       " ('Collection', 0.4234597086906433),\n",
       " ('collection', 0.4215546250343323),\n",
       " ('Frigate', 0.4143460988998413)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive='艦娘',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-23 19:35:44,629 : INFO : loading projection weights from D:\\Backup\\ml_data\\Data_set\\NLP\\Tencent_AILab_ChineseEmbedding\\Tencent_AILab_ChineseEmbedding.txt\n",
      "2019-03-23 19:39:56,312 : INFO : loaded (2000000, 200) matrix from D:\\Backup\\ml_data\\Data_set\\NLP\\Tencent_AILab_ChineseEmbedding\\Tencent_AILab_ChineseEmbedding.txt\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from ML_utils import ML_utils\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "word2vec_modle_path = r\"D:\\Backup\\ml_data\\Data_set\\NLP\\Tencent_AILab_ChineseEmbedding\\Tencent_AILab_ChineseEmbedding.txt\"\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(word2vec_modle_path, binary=False,limit=2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-23 19:40:38,858 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "wv_from_text.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhin\\AppData\\Local\\conda\\conda\\envs\\torch1\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('quartet', 0.6501891016960144),\n",
       " (',,,', 0.6361609697341919),\n",
       " (',', 0.633293867111206),\n",
       " ('what', 0.6222460269927979),\n",
       " (\"what's\", 0.614862322807312)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text.wv.most_similar(positive='?',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from ML_utils import ML_utils\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\jieba_dict\\新文字文件.txt\"\n",
    "d2 = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\jieba_dict\\新文字文件 (2).txt\"\n",
    "with open(d1,'r',encoding='utf-8') as f1:\n",
    "    with open(d2,'r',encoding='utf-8') as f2:\n",
    "        set1 = set([i.strip('\\n') for i in f1])\n",
    "        set2 = set([i.strip('\\n') for i in f2])\n",
    "        set3 = set.intersection(set1,set2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85619"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d3 = \"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\jieba_dict\\dict2.txt - 複製.big\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(d3,sep=' ',header=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_index = data[data.iloc[:,0].isin(set3)].index\n",
    "new_data = data.drop(index=drop_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d4 = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\jieba_dict\\new_dict2.txt.big\"\n",
    "new_data.to_csv(d4,sep=' ',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
