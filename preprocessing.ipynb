{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ML_utils import ML_utils\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import shutil\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import logging\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train wordvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\merge_wiki.txt\"\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\merge_wiki_tw.txt\"\n",
    "ML_utils.Opencc_file(file_path,save_path,replace_old=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userdict_path = './jieba_dict/ptt_userdict.txt'\n",
    "jieba_dict = 'jieba_dict/jieba_merge.txt'\n",
    "dir_path = r\"./word2vec_model\"\n",
    "ML_utils.CallF_DirFile_save(dir_path, ML_utils.Jieba_file_segmentation, \n",
    "                            replace_old=False,regular=True,file_filter_=None,\n",
    "                            dict_path=jieba_dict,load_userdict_path=userdict_path,HMM=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\"\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec\"\n",
    "ML_utils.Word2vec_train(file_path=None,dir_path=dir_path,save_path=save_path,model_min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec\"\n",
    "model = word2vec.Word2Vec.load(save_path)\n",
    "model.wv.most_similar(positive='台灣',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14634 -3.2558982\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 2.19233370e+00, -1.46388721e+00, -1.99675608e+00,  6.79616570e-01,\n",
       "        3.55416924e-01,  1.59947169e+00,  2.12003097e-01,  1.02245383e-01,\n",
       "       -6.30102754e-02,  1.45580783e-01, -1.20840502e+00,  1.05694366e+00,\n",
       "       -4.72883910e-01, -7.30408907e-01, -6.62375212e-01,  3.69489416e-02,\n",
       "        6.03886724e-01, -1.04588306e+00, -9.98719633e-01,  4.87945765e-01,\n",
       "       -1.19801438e+00,  5.29940845e-03, -5.62464714e-01,  9.90700066e-01,\n",
       "       -1.11428118e+00,  2.33197063e-02, -4.84015942e-01, -4.22614723e-01,\n",
       "        1.55693221e+00, -8.42484772e-01,  1.12706840e+00, -4.90916669e-01,\n",
       "       -1.82026911e+00,  1.78608343e-01,  1.95130181e+00, -5.35292745e-01,\n",
       "       -2.13389695e-01, -4.45280492e-01, -6.62543356e-01, -2.24680066e-01,\n",
       "       -8.58971775e-01,  3.13481003e-01, -4.12401855e-01,  8.42198312e-01,\n",
       "        8.90280306e-01, -2.93085545e-01,  5.73269837e-02,  5.64253807e-01,\n",
       "        1.94658458e-01,  7.06532478e-01, -9.69610810e-01, -2.71015286e-01,\n",
       "       -1.37021288e-01,  1.10866928e+00, -9.07493472e-01,  3.14584732e-01,\n",
       "        4.12069596e-02,  1.31585419e+00, -8.09865594e-01, -1.26368296e+00,\n",
       "       -1.11141109e+00,  2.81190276e-01,  1.20582724e+00, -2.27073450e-02,\n",
       "       -1.03319871e+00,  1.25526503e-01, -1.47554147e+00, -8.21148098e-01,\n",
       "        1.43459535e+00, -1.15120336e-01, -1.69880584e-01,  2.97379285e-01,\n",
       "        9.83806420e-03,  9.31956708e-01,  6.95895731e-01, -1.29935607e-01,\n",
       "        8.44273150e-01, -1.68290555e+00, -2.26361841e-01,  1.94219792e+00,\n",
       "       -8.94085407e-01, -1.11553526e+00,  2.20188975e+00, -6.93827689e-01,\n",
       "        3.35767806e-01,  5.90315878e-01, -2.11028028e+00,  3.56898248e-01,\n",
       "        1.55220604e+00,  8.27677131e-01,  1.18238136e-01, -6.17022216e-01,\n",
       "       -1.75103748e+00,  1.78179538e+00,  7.51693428e-01,  1.39127231e+00,\n",
       "        1.16996251e-01, -1.71421754e+00,  3.90322924e-01,  6.85285687e-01,\n",
       "       -3.63789946e-02, -8.62000167e-01, -3.96399619e-03, -1.65105081e+00,\n",
       "       -8.75971794e-01,  6.03116512e-01, -6.21975243e-01,  4.68084008e-01,\n",
       "        3.39009434e-01, -2.09049657e-01,  1.23967755e+00,  2.18381286e+00,\n",
       "       -5.82348704e-01, -1.60677230e+00,  9.38999429e-02,  7.88921654e-01,\n",
       "        9.12856936e-01, -9.96641755e-01,  8.95208538e-01,  1.70566428e+00,\n",
       "       -1.11710086e-01, -8.02423775e-01, -5.90756461e-02, -8.61479700e-01,\n",
       "       -3.51199180e-01,  1.01564252e+00,  5.05317330e-01,  2.28152084e+00,\n",
       "        3.50865752e-01,  4.23168063e-01, -1.83592185e-01, -7.49524593e-01,\n",
       "       -1.94670188e+00, -5.52511036e-01, -4.87413704e-01, -1.61471462e+00,\n",
       "       -1.76942897e+00, -8.17228377e-01, -1.01038910e-01,  1.08588743e+00,\n",
       "       -4.74637061e-01,  2.48071030e-01,  9.65371802e-02,  5.24936942e-03,\n",
       "       -4.40657437e-01, -5.99854171e-01,  1.47170866e+00, -1.44029188e+00,\n",
       "       -2.62879848e-01,  2.00500298e+00,  1.27408934e+00,  1.08712614e+00,\n",
       "        1.08781111e+00, -2.23920345e+00, -9.03347909e-01,  5.93493760e-01,\n",
       "       -1.94508806e-01, -4.65428047e-02,  8.14721107e-01,  8.22978497e-01,\n",
       "       -1.11793481e-01, -1.26302376e-01,  2.51943469e-01, -9.57435220e-02,\n",
       "        5.74462891e-01, -2.14255080e-01,  2.19279456e+00,  8.37389946e-01,\n",
       "       -1.79754114e+00, -6.69074714e-01,  4.15443569e-01, -3.38161439e-02,\n",
       "       -1.18240702e+00,  1.76874185e+00,  1.00343235e-01, -1.73865783e+00,\n",
       "        1.96232185e-01, -1.09829688e+00, -1.60410547e+00, -1.74677029e-01,\n",
       "        4.59089696e-01,  1.47642136e+00, -2.10924625e+00, -3.25589824e+00,\n",
       "        1.06921576e-01, -9.59845126e-01, -5.50160050e-01, -1.52367985e+00,\n",
       "        3.87737095e-01,  3.95829082e-01,  1.08888316e+00, -1.95509866e-01,\n",
       "        1.28156018e+00,  2.29805303e+00,  7.59883225e-01,  2.64029533e-01,\n",
       "        8.06451797e-01, -4.85674620e-01, -1.46031356e+00, -5.75003028e-01,\n",
       "       -2.09708020e-01, -5.24419844e-01,  2.00433469e+00,  2.48809233e-01,\n",
       "        5.74118376e-01, -1.83304970e-03, -1.46190214e+00,  1.26513171e+00,\n",
       "       -1.54643643e+00, -4.04618710e-01,  3.06522727e-01,  1.31315958e+00,\n",
       "       -9.83127832e-01, -3.69540215e-01, -1.92226395e-01, -1.10487914e+00,\n",
       "        1.53027165e+00, -1.23666719e-01, -5.53876936e-01, -5.80529690e-01,\n",
       "       -2.73889089e+00, -1.60561430e+00, -1.31140947e+00,  9.20682371e-01,\n",
       "       -1.11339784e+00,  4.85417664e-01,  7.41955280e-01, -1.25789762e+00,\n",
       "        1.06256080e+00, -1.47921312e+00,  9.53109384e-01, -1.05467165e+00,\n",
       "       -2.02103400e+00, -1.85646266e-01,  8.58556211e-01,  3.14633989e+00,\n",
       "        7.70402551e-01, -2.11313701e+00, -1.06216168e+00,  6.46613061e-01,\n",
       "       -5.06923795e-02, -2.33915955e-01, -1.36622584e+00, -1.58399493e-01,\n",
       "       -1.37342095e-01,  1.53338397e+00, -1.77101302e+00, -2.09668100e-01,\n",
       "        1.15503073e+00, -2.30852032e+00,  2.38671517e+00,  2.22373080e+00,\n",
       "        2.50376880e-01,  6.32725537e-01, -1.25257134e+00, -8.83007050e-01,\n",
       "        1.61096835e+00, -4.41857576e-01, -7.00694621e-01, -6.76744819e-01,\n",
       "       -8.49156976e-01, -3.33193004e-01, -2.06874013e+00, -3.60488772e-01,\n",
       "        3.71970832e-01, -3.08958501e-01,  5.78803904e-02,  2.83212471e+00,\n",
       "        1.11615765e+00, -7.34549165e-01, -1.09843373e+00,  1.45100921e-01,\n",
       "       -4.33030188e-01, -3.96836102e-01,  5.73138036e-02, -1.77957892e+00,\n",
       "       -1.30462420e+00,  1.74620643e-01, -3.43259394e-01,  6.46397889e-01,\n",
       "        2.60970020e+00, -1.82819247e-01, -1.01732659e+00, -4.41915989e-01,\n",
       "       -1.28523791e+00, -1.17458761e+00,  5.06800592e-01, -1.46551561e+00,\n",
       "       -2.43649244e+00, -6.21503711e-01, -1.44017065e+00, -1.24973953e+00,\n",
       "        3.08079928e-01,  7.98492320e-03, -9.13577378e-01, -2.17784595e+00,\n",
       "        1.71819544e+00, -1.91348052e+00, -1.36009622e+00,  1.78620592e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.wv.get_vector('')\n",
    "print(max(a),min(a))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec add token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add token\n",
    "model_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec\"\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec1\"\n",
    "train_file_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\jieba_dict\\token\\token.txt\"\n",
    "ML_utils.add_word2vec_word(model_path,save_path,train_file_path=train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec1\"\n",
    "model = word2vec.Word2Vec.load(save_path)\n",
    "model.wv.most_similar(positive='__EOS.Token__',topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data add token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\data\\merge_ptt.txt\"\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\data\\merge_ptt_token.txt\"\n",
    "ML_utils.data_add_token_SOS_EOS(data_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data jieba segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\data\\merge_ptt_token.txt\"\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\data\\merge_ptt_token_jieba.txt\"\n",
    "load_userdict_path = './jieba_dict/ptt_userdict.txt'\n",
    "dict_path = 'jieba_dict/jieba_merge.txt'\n",
    "ML_utils.Jieba_file_segmentation(data_path,save_path,\n",
    "                                 dict_path=dict_path,load_userdict_path=load_userdict_path,HMM=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add loss_word to wordvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Null_to_space(list_in):\n",
    "    index = []\n",
    "    for num in range(len(list_in)):        \n",
    "        remove_num = 0\n",
    "        if list_in[num] == \"\" or list_in[num] == \"\\u3000\":\n",
    "            index.append(num)\n",
    "    for num in range(len(index)):\n",
    "        remove_index = index[num]-remove_num\n",
    "        list_in.pop(remove_index)\n",
    "        remove_num = remove_num+1\n",
    "    return list_in\n",
    "\n",
    "def get_pair_vec(model,pairs,num):\n",
    "    pair = pairs[num]\n",
    "    X = pair[0]\n",
    "    Y = pair[1]\n",
    "    loss_word = []\n",
    "    try:\n",
    "        for word in X:\n",
    "            model.wv.get_vector(word)\n",
    "        for word in Y:\n",
    "            model.wv.get_vector(word)\n",
    "    except:\n",
    "        loss_word.append(word)\n",
    "    return loss_word\n",
    "\n",
    "data_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\data\\merge_ptt_token_jieba.txt\"\n",
    "pairs = []\n",
    "with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
    "    i = 0\n",
    "    for line in f:\n",
    "        pair = line.split(\"\\t\")\n",
    "        X = pair[0].strip(\" \").split(\" \")\n",
    "        X = Null_to_space(X)\n",
    "        Y = pair[1].strip(\"\\n\").strip(\" \").split(\" \")\n",
    "        Y = Null_to_space(Y)\n",
    "        pairs.append([X,Y])\n",
    "\n",
    "model_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec1\"\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec1\"\n",
    "model = word2vec.Word2Vec.load(model_path)\n",
    "loss_word_set = set()\n",
    "for num in range(len(pairs)):\n",
    "    loss_word = get_pair_vec(model,pairs,num)\n",
    "    loss_word_set.update(loss_word)\n",
    "\n",
    "sentences = []   \n",
    "for word in list(loss_word_set):\n",
    "    sentences.append([word])\n",
    "print(sentences)    \n",
    "model.vocabulary.min_count = 1\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "model.build_vocab(sentences, update=True)#训练该行\n",
    "model.train(sentences,total_examples= model.corpus_count,epochs= model.epochs)\n",
    "model.save(save_path)\n",
    "model.wv.most_similar(positive=sentences[0],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# wordvec save keyvector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\model_wordvec1\"\n",
    "save_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\keyvector\\model_keyvector\"\n",
    "ML_utils.wordvec2keyvector(model_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "model_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\word2vec_model\\keyvector\\model_keyvector\"\n",
    "word_vectors = KeyedVectors.load(model_path, mmap='r')\n",
    "test = word_vectors.get_vector(\"hello\")\n",
    "word_vectors.similar_by_vector(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test clip data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_space(list_in):\n",
    "    index = []\n",
    "    for num in range(len(list_in)):        \n",
    "        remove_num = 0\n",
    "        if list_in[num] == \"\" or list_in[num] == \"\\u3000\":\n",
    "            index.append(num)\n",
    "    for num in range(len(index)):\n",
    "        remove_index = index[num]-remove_num\n",
    "        list_in.pop(remove_index)\n",
    "        remove_num = remove_num+1\n",
    "    return list_in\n",
    "\n",
    "def file_to_word_list(data_path):\n",
    "    pairs = []\n",
    "    with open(data_path,\"r\",encoding=\"utf-8\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            pair = line.split(\"\\t\")\n",
    "            X = pair[0].strip(\" \").split(\" \")\n",
    "            X = del_space(X)\n",
    "            Y = pair[1].strip(\"\\n\").strip(\" \").split(\" \")\n",
    "            Y = del_space(Y)\n",
    "            pairs.append([X,Y])\n",
    "            if len(X) > 22:\n",
    "                print(X)\n",
    "            if len(Y) > 22:\n",
    "                print(Y)\n",
    "    return pairs\n",
    "data_path = r\"D:\\Backup\\ml_data\\GitHub\\chat-robot_seq2seq\\data\\merge_ptt_token_jieba.txt\"\n",
    "pairs = file_to_word_list(data_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
