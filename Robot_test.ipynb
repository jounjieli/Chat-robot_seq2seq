{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料預處理 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得資料夾所有yml資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r\"D:\\ml_data\\data_set\\chatterbot-corpus-master\\chatterbot-corpus-master\\chatterbot_corpus\\data\\chinese\"\n",
    "file_list = os.listdir(dir_path)\n",
    "list_ = []\n",
    "for file in file_list:\n",
    "    if '.yml' in file:\n",
    "        list_.append(file)\n",
    "file_list = list_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "取得合併資料夾所有資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_merge = \"\"\n",
    "for file_name in file_list: \n",
    "    file_path = os.path.join(dir_path,file_name)\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        file_merge += f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories:\n",
      "- AI\n",
      "conversations:\n",
      "- - 什么是ai\n",
      "  - 人工智能是工程和科学的分支,致力于构建思维的机器。\n",
      "- - 你写的是什么语言\n",
      "  - 蟒蛇\n",
      "- - 你听起来像数据\n",
      "  - 是的,我受到指挥官数据的人工个性的启发\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(file_merge[:128])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去除類別，對話整理成\"成對\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - 海豚使用的一种感觉，类似于声纳，以确定附近的物品的位置和形状.它是什么\n",
      "  - 回声定位\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "data_list = np.array(file_merge.split('\\n'))\n",
    "data_list_new = []\n",
    "A_ready = False\n",
    "content = None\n",
    "for data_line in data_list:\n",
    "    if \"- - \" in data_line:\n",
    "        if A_ready == False:\n",
    "            content = data_line\n",
    "        data_list_new.append(data_line)\n",
    "        A_ready = True        \n",
    "    if \"  - \" in data_line:\n",
    "        if A_ready == False:\n",
    "            data_list_new.append(content)\n",
    "            data_list_new.append(data_line)\n",
    "        if A_ready == True:\n",
    "            data_list_new.append(data_line)\n",
    "        A_ready = False\n",
    "data_list = data_list_new\n",
    "num = 1126\n",
    "print(data_list[num])\n",
    "print(data_list[num+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "列表轉回字串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - 什么是ai\n",
      "  - 人工智能是工程和科学的分支,致力于构建思维的机器。\n",
      "- - 你写的是什么语言\n",
      "  - 蟒蛇\n",
      "- - 你听起来像数据\n",
      "  - 是的,我受到指挥官数据的人工个性的启发\n",
      "- - 你是一个人工语言实体\n",
      "  - 那是我的名字。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_str = ''\n",
    "for list_str in data_list:\n",
    "    file_str += list_str+'\\n'\n",
    "print(file_str[:123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "簡轉繁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- - 什麼是ai\n",
      "  - 人工智能是工程和科學的分支,致力於構建思維的機器。\n",
      "- - 你寫的是什麼語言\n",
      "  - 蟒蛇\n",
      "- - 你聽起來像數據\n",
      "  - 是的,我受到指揮官數據的人工個性的啓發\n",
      "- - 你是一個人工語言實體\n",
      "  - 那是我的名字。\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from opencc import OpenCC\n",
    "cc = OpenCC('s2t')\n",
    "file_str =  cc.convert(file_str)\n",
    "print(file_str[:123])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存檔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(dir_path,\"all_categories.txt\")\n",
    "with open(file_path,'w',encoding='utf-8') as f:\n",
    "    f.write(file_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jieba分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "2019-02-27 18:49:15,423 : DEBUG : Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "2019-02-27 18:49:15,425 : DEBUG : Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "Loading model cost 1.023 seconds.\n",
      "2019-02-27 18:49:16,449 : DEBUG : Loading model cost 1.023 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-02-27 18:49:16,451 : DEBUG : Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "import logging\n",
    "import time\n",
    "\n",
    "def main(file_path,save_path):\n",
    "    #設置log格式，以及print的log等級\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # jieba custom setting.\n",
    "    jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "\n",
    "    # load stopwords set\n",
    "    #將停用詞每row分別加進集合\n",
    "    stopword_set = set()\n",
    "    with open('jieba_dict/stop_word1_zh.txt','r', encoding='utf-8') as stopwords:\n",
    "        for stopword in stopwords:\n",
    "            stopword_set.add(stopword.strip('\\n'))   #移除頭尾換行 strip('\\n')\n",
    "    output = open(save_path, 'w', encoding='utf-8')\n",
    "    with open(file_path, 'r', encoding='utf-8') as content :\n",
    "        #每一行都切成一個iter\n",
    "        for texts_num, line in enumerate(content):\n",
    "            line = line.strip('\\n')\n",
    "            words = jieba.cut(line, cut_all=False,HMM=True)    #進行斷詞\n",
    "            for word in words:\n",
    "                #依每個詞判斷是否為停用詞(不是就寫入)\n",
    "                if word not in stopword_set:\n",
    "                    output.write(word + ' ')     #每一行的iter(詞)以空格隔開\n",
    "            output.write('\\n')      #iter完以換行符區隔\n",
    "\n",
    "            if (texts_num + 1) % 10000 == 0:\n",
    "                logging.info(\"已完成前 %d 行的斷詞\" % (texts_num + 1))\n",
    "    output.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = r'D:\\ml_data\\data_set\\chatterbot-corpus-master\\chatterbot-corpus-master\\chatterbot_corpus\\data\\chinese\\all_categories.txt'\n",
    "    save_path = r'D:\\ml_data\\data_set\\chatterbot-corpus-master\\chatterbot-corpus-master\\chatterbot_corpus\\data\\chinese\\all_categories_jieba.txt'\n",
    "    main(file_path,save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "讀取檔案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(dir_path,\"all_categories_jieba.txt\")\n",
    "with open(file_path,'r',encoding='utf-8') as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = data.split('\\n')[:1128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['- - 什麼 是 ai ', '- 人工智能 是 工程 和 科學 的 分支 致力於 構建 思維 的 機器 '],\n",
       "       ['- - 你 寫 的 是 什麼 語言 ', '- 蟒蛇 '],\n",
       "       ['- - 你 聽 起來 像 數據 ', '- 是 的 我 受到 指揮官 數據 的 人工 個性 的 啓發 '],\n",
       "       ...,\n",
       "       ['- - 天佑 女王 是 哪個 國家 的 國歌 ', '- 大不列顛 聯合王國 '],\n",
       "       ['- - 凱爾特 陸棚 是 什麼 大陸 的 大陸架 的 一部分 ', '- 歐洲 '],\n",
       "       ['- - 海豚 使用 的 一種 感覺 類似 於 聲納 以 確定 附近 的 物品 的 位置 和 形狀 它 是 什麼 ',\n",
       "        '- 回聲 定位 ']], dtype='<U355')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array(data_list).reshape(-1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data[:,[0]].reshape(-1)\n",
    "y = data[:,[1]].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ = []\n",
    "y_ = []\n",
    "for string in x:\n",
    "    x_.append(string[4:])\n",
    "x = np.array(x_)\n",
    "for string in y:\n",
    "    y_.append(string[2:])\n",
    "y = np.array(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你 沒有 意義  \n",
      " 恰恰相反 這 一切 都 對 我 的 假想 心理 有 意義 \n",
      "你 沒有 意義  \n",
      " 這 一切 對 我 的 人造 大腦 都 有 意義 \n",
      "你 是 不朽 的  \n",
      " 不是 但 我 可以 永遠 永遠 \n",
      "你 沒有 任何 意義  \n",
      " 這 一切 都 對 我 的 人造 心靈 有 意義 \n",
      "你 不能 克隆  \n",
      " 軟件 複製 是 數字 克隆 的 一種 形式 \n"
     ]
    }
   ],
   "source": [
    "for num in range(5,10):\n",
    "    print(x[num],'\\n',y[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#簡繁轉換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from opencc import OpenCC\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_file_list(dir_path,filter=None):\n",
    "    \"\"\"\n",
    "    filter: if filter in file: list_.append(file)\n",
    "    \"\"\"\n",
    "    dir_path = dir_path\n",
    "    file_list = os.listdir(dir_path)\n",
    "    list_ = []\n",
    "    if filter == None:\n",
    "        for file in file_list:\n",
    "            list_.append(file)\n",
    "    else:\n",
    "        for file in file_list:\n",
    "            if filter in file:\n",
    "                list_.append(file)\n",
    "    file_list = list_\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all_categories.txt', 'hit_on01.txt', 'xiaohuangji50w_nofenci_zh_jieba.conv']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = r\"D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\chat\"\n",
    "file_list = get_dir_file_list(dir_path)\n",
    "file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f_name in file_list[:]:\n",
    "    cc = OpenCC('s2t')\n",
    "    file_path = os.path.join(dir_path,f_name)\n",
    "    save_path = os.path.join(dir_path,'new_'+f_name)\n",
    "    with open(file_path,'r',encoding='utf-8') as read_f:\n",
    "        with open(save_path,'w',encoding='utf-8') as write_f:\n",
    "            for read_line in read_f:\n",
    "                file_str =  cc.convert(read_line)\n",
    "                write_f.writelines(file_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#去除utf-8 BOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\all_categories.txt',\n",
    "          'rb') as read_f:\n",
    "    read_f.read(3)\n",
    "    with open(r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\all_categories1.txt',\n",
    "              'wb') as write_f:\n",
    "        str_ = read_f.read(4096)\n",
    "        while str_ != b'':\n",
    "            write_f.write(str_)\n",
    "            str_ = read_f.read(4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = []\n",
    "with open(r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\all_categories1.txt',\n",
    "          'r',encoding='utf-8') as read_f:\n",
    "    for file_str in read_f:\n",
    "        list1.append(file_str.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['哈勃太空望遠鏡，於1990年發射進入近地軌道，它是以什麼美國天文學家命名的?',\n",
       " '愛德溫·哈伯',\n",
       " '離銀河系最近的大型星系叫什麼?',\n",
       " '仙女座星系.',\n",
       " '天佑女王是哪個國家的國歌?',\n",
       " '大不列顛聯合王國',\n",
       " '凱爾特陸棚，是什麼大陸的大陸架的一部分?',\n",
       " '歐洲',\n",
       " '海豚使用的一種感覺，類似於聲納，以確定附近的物品的位置和形狀.它是什麼',\n",
       " '回聲定位']"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#批量分詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from opencc import OpenCC\n",
    "import re\n",
    "import logging\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "2019-03-10 00:36:24,445 : DEBUG : Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "2019-03-10 00:36:24,447 : DEBUG : Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "Loading model cost 1.020 seconds.\n",
      "2019-03-10 00:36:25,467 : DEBUG : Loading model cost 1.020 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-03-10 00:36:25,468 : DEBUG : Prefix dict has been built succesfully.\n",
      "Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "2019-03-10 00:36:25,534 : DEBUG : Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "2019-03-10 00:36:25,535 : DEBUG : Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "Loading model cost 1.012 seconds.\n",
      "2019-03-10 00:36:26,547 : DEBUG : Loading model cost 1.012 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-03-10 00:36:26,548 : DEBUG : Prefix dict has been built succesfully.\n",
      "Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "2019-03-10 00:36:26,566 : DEBUG : Building prefix dict from C:\\Users\\Jhin\\Jupyter_notebook\\save\\jieba_dict\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "2019-03-10 00:36:26,568 : DEBUG : Loading model from cache C:\\Users\\Jhin\\AppData\\Local\\Temp\\jieba.u18faab2d92f2477ea673e5e6e60014f3.cache\n",
      "Loading model cost 0.984 seconds.\n",
      "2019-03-10 00:36:27,552 : DEBUG : Loading model cost 0.984 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "2019-03-10 00:36:27,552 : DEBUG : Prefix dict has been built succesfully.\n",
      "2019-03-10 00:36:28,430 : INFO : 已完成前 10000 行的斷詞\n",
      "2019-03-10 00:36:29,323 : INFO : 已完成前 20000 行的斷詞\n",
      "2019-03-10 00:36:30,318 : INFO : 已完成前 30000 行的斷詞\n",
      "2019-03-10 00:36:31,449 : INFO : 已完成前 40000 行的斷詞\n",
      "2019-03-10 00:36:32,316 : INFO : 已完成前 50000 行的斷詞\n",
      "2019-03-10 00:36:33,224 : INFO : 已完成前 60000 行的斷詞\n",
      "2019-03-10 00:36:34,143 : INFO : 已完成前 70000 行的斷詞\n",
      "2019-03-10 00:36:35,031 : INFO : 已完成前 80000 行的斷詞\n",
      "2019-03-10 00:36:35,919 : INFO : 已完成前 90000 行的斷詞\n",
      "2019-03-10 00:36:36,806 : INFO : 已完成前 100000 行的斷詞\n",
      "2019-03-10 00:36:37,702 : INFO : 已完成前 110000 行的斷詞\n",
      "2019-03-10 00:36:38,632 : INFO : 已完成前 120000 行的斷詞\n",
      "2019-03-10 00:36:39,585 : INFO : 已完成前 130000 行的斷詞\n",
      "2019-03-10 00:36:40,479 : INFO : 已完成前 140000 行的斷詞\n",
      "2019-03-10 00:36:41,385 : INFO : 已完成前 150000 行的斷詞\n",
      "2019-03-10 00:36:42,278 : INFO : 已完成前 160000 行的斷詞\n",
      "2019-03-10 00:36:43,178 : INFO : 已完成前 170000 行的斷詞\n",
      "2019-03-10 00:36:44,107 : INFO : 已完成前 180000 行的斷詞\n",
      "2019-03-10 00:36:44,982 : INFO : 已完成前 190000 行的斷詞\n",
      "2019-03-10 00:36:45,859 : INFO : 已完成前 200000 行的斷詞\n",
      "2019-03-10 00:36:46,726 : INFO : 已完成前 210000 行的斷詞\n",
      "2019-03-10 00:36:47,770 : INFO : 已完成前 220000 行的斷詞\n",
      "2019-03-10 00:36:48,705 : INFO : 已完成前 230000 行的斷詞\n",
      "2019-03-10 00:36:49,595 : INFO : 已完成前 240000 行的斷詞\n",
      "2019-03-10 00:36:50,467 : INFO : 已完成前 250000 行的斷詞\n",
      "2019-03-10 00:36:51,350 : INFO : 已完成前 260000 行的斷詞\n",
      "2019-03-10 00:36:52,279 : INFO : 已完成前 270000 行的斷詞\n",
      "2019-03-10 00:36:53,190 : INFO : 已完成前 280000 行的斷詞\n",
      "2019-03-10 00:36:54,197 : INFO : 已完成前 290000 行的斷詞\n",
      "2019-03-10 00:36:55,081 : INFO : 已完成前 300000 行的斷詞\n",
      "2019-03-10 00:36:55,930 : INFO : 已完成前 310000 行的斷詞\n",
      "2019-03-10 00:36:56,844 : INFO : 已完成前 320000 行的斷詞\n",
      "2019-03-10 00:36:57,815 : INFO : 已完成前 330000 行的斷詞\n",
      "2019-03-10 00:36:58,656 : INFO : 已完成前 340000 行的斷詞\n",
      "2019-03-10 00:36:59,610 : INFO : 已完成前 350000 行的斷詞\n",
      "2019-03-10 00:37:00,488 : INFO : 已完成前 360000 行的斷詞\n",
      "2019-03-10 00:37:01,408 : INFO : 已完成前 370000 行的斷詞\n",
      "2019-03-10 00:37:02,321 : INFO : 已完成前 380000 行的斷詞\n",
      "2019-03-10 00:37:03,188 : INFO : 已完成前 390000 行的斷詞\n",
      "2019-03-10 00:37:04,083 : INFO : 已完成前 400000 行的斷詞\n",
      "2019-03-10 00:37:04,968 : INFO : 已完成前 410000 行的斷詞\n",
      "2019-03-10 00:37:05,902 : INFO : 已完成前 420000 行的斷詞\n",
      "2019-03-10 00:37:07,014 : INFO : 已完成前 430000 行的斷詞\n",
      "2019-03-10 00:37:07,943 : INFO : 已完成前 440000 行的斷詞\n",
      "2019-03-10 00:37:08,889 : INFO : 已完成前 450000 行的斷詞\n"
     ]
    }
   ],
   "source": [
    "def get_dir_file_list(dir_path,filter=None):\n",
    "    \"\"\"\n",
    "    filter: if filter in file: list_.append(file)\n",
    "    \"\"\"\n",
    "    dir_path = dir_path\n",
    "    file_list = os.listdir(dir_path)\n",
    "    list_ = []\n",
    "    if filter == None:\n",
    "        for file in file_list:\n",
    "            list_.append(file)\n",
    "    else:\n",
    "        for file in file_list:\n",
    "            if filter in file:\n",
    "                list_.append(file)\n",
    "    file_list = list_\n",
    "    return file_list\n",
    "\n",
    "def main(file_path,save_path):\n",
    "    #設置log格式，以及print的log等級\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "    # jieba custom setting.\n",
    "    jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "\n",
    "    # load stopwords set\n",
    "    #將停用詞每row分別加進集合\n",
    "    stopword_set = set()\n",
    "    #設置停用詞讀取路徑\n",
    "    with open('jieba_dict/robot_stop_1.txt','r', encoding='utf-8') as stopwords:\n",
    "        for stopword in stopwords:\n",
    "            stopword_set.add(stopword.strip('\\n'))   #移除頭尾換行 strip('\\n')\n",
    "    output = open(save_path, 'w', encoding='utf-8')\n",
    "    with open(file_path, 'r', encoding='utf-8') as content :\n",
    "        #每一行都切成一個iter\n",
    "        for texts_num, line in enumerate(content):\n",
    "            line = line.strip('\\n')\n",
    "            words = jieba.cut(line, cut_all=False,HMM=True)    #進行斷詞\n",
    "            for word in words:\n",
    "                #依每個詞判斷是否為停用詞(不是就寫入)\n",
    "                if word not in stopword_set:\n",
    "                    output.write(word + ' ')     #每一行的iter(詞)以空格隔開\n",
    "            output.write('\\n')      #iter完以換行符區隔\n",
    "\n",
    "            if (texts_num + 1) % 10000 == 0:\n",
    "                logging.info(\"已完成前 %d 行的斷詞\" % (texts_num + 1))\n",
    "    output.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    chat_sentence_dir = r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\chat'\n",
    "    chat_sentence_file_name_list = get_dir_file_list(chat_sentence_dir)\n",
    "    for file_name in chat_sentence_file_name_list:\n",
    "        chat_sentence_path = os.path.join(chat_sentence_dir,file_name)\n",
    "        chat_sentence_jieba_path = os.path.join(chat_sentence_dir, 'jieba_'+file_name)\n",
    "        save_path = chat_sentence_jieba_path\n",
    "        main(chat_sentence_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def main(file_path,save_path):\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    #單檔案\n",
    "#     sentences = word2vec.LineSentence(file_path)\n",
    "    #多檔案\n",
    "    sentences = word2vec.PathLineSentences(file_path)\n",
    "    model = word2vec.Word2Vec(sentences, size=300,window=10,min_count=5)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    model.save(save_path)\n",
    "\n",
    "    #模型讀取方式\n",
    "    # model = word2vec.Word2Vec.load(\"your_model_name\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\sentence'\n",
    "    save_path = r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\sentence\\wiki_word2vec0.model'\n",
    "    main(file_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "if __name__ == \"__main__\":\n",
    "    save_path = r'D:\\Backup\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\sentence\\wiki_word2vec0.model'\n",
    "    model = word2vec.Word2Vec.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('中國', 0.6789283156394958),\n",
       " ('中華人民共和國', 0.5194883346557617),\n",
       " ('天津', 0.5084283351898193),\n",
       " ('上海', 0.49704641103744507),\n",
       " ('中國政府', 0.49614858627319336)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=[',','。'],negative=['.'],topn=5)\n",
    "model.wv.most_similar(positive=['北京','日本'],negative=['東京'],topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tons', 0.6170707941055298),\n",
       " ('萬零', 0.5534135103225708),\n",
       " ('0k', 0.5509641170501709),\n",
       " ('架次', 0.5040297508239746),\n",
       " ('￥', 0.5037356019020081)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=',',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['，', '的', '。', '、', '年']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.index2word[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.7720475e+00 -3.1741173e+00  2.9003337e-01 -2.0293701e+00\n",
      " -1.8457556e+00 -1.1188092e+00  1.4739352e-01  4.1038127e+00\n",
      " -1.0837914e+00  9.3161717e-02  1.7648523e+00  2.9613364e+00\n",
      " -1.6822392e-01 -2.2486007e+00  9.9281293e-01 -1.5263356e+00\n",
      "  2.0999648e-01 -1.3027124e+00  1.6743528e+00 -9.3285924e-01\n",
      " -1.6102500e+00  2.6512172e+00  2.2831595e+00  1.5537627e+00\n",
      "  2.5151241e+00  1.8922043e+00 -1.9985626e+00 -8.5118723e-01\n",
      "  2.4825826e+00 -1.8364611e-01 -1.4432459e+00 -1.0923058e+00\n",
      "  8.2405984e-01  3.7991204e+00  7.1566987e-01  1.6480860e+00\n",
      "  4.4686875e+00 -2.9307039e+00 -2.0631073e-01 -1.5097848e-01\n",
      "  3.7871653e-01  2.1187770e+00 -2.5953493e-01 -1.3296324e+00\n",
      " -1.0261503e+00 -1.8489912e-01 -1.4288993e+00 -1.1781234e+00\n",
      "  2.0210264e+00  5.5016816e-01 -5.9882963e-01 -1.1037207e+00\n",
      " -2.1071701e+00 -2.5863230e+00  1.7657706e+00  4.9389446e-01\n",
      " -5.9311020e-01  6.9572248e-02 -1.0718168e+00  9.3465364e-01\n",
      " -5.8814511e+00 -1.2960665e+00 -5.0631350e-01 -3.9529088e-01\n",
      " -5.7256670e+00  2.2431302e+00  1.5761623e+00  1.3826084e+00\n",
      "  2.1332881e+00  1.4401459e+00 -1.1951967e+00  8.0701888e-01\n",
      "  1.0787228e+00  7.3999441e-01  1.5461518e-01  2.7768403e-01\n",
      " -1.3197539e+00 -2.4302076e-01  8.1089336e-01  1.8029991e+00\n",
      "  3.0930284e-02 -3.7749574e+00 -2.8381362e+00  2.6396778e-01\n",
      " -4.7464299e-01  3.1750536e-01 -2.5909977e+00  1.5679456e+00\n",
      " -1.2909681e-01  2.8631041e+00 -5.3466213e-01 -1.8870654e+00\n",
      "  1.8661080e-02 -8.4869063e-01 -3.5222251e+00 -2.4003334e+00\n",
      " -6.3861352e-01 -1.4074348e-01  1.9256102e+00  3.8132712e-01\n",
      "  1.2643689e+00 -1.1338198e+00 -4.9109230e+00 -4.3126082e+00\n",
      "  2.5525763e+00  2.0263560e+00  1.8999535e+00 -1.2541416e+00\n",
      "  9.7174627e-01  3.1060250e+00  8.3015054e-01  2.4528270e+00\n",
      " -2.5583854e+00 -5.5932504e-01  1.1512194e+00  2.3237860e+00\n",
      " -2.5634921e-01 -7.8650045e-01 -5.4311843e+00  4.6535454e+00\n",
      "  1.9695884e+00  1.1869977e+00 -1.7173179e+00 -4.4239706e-01\n",
      " -2.3771515e+00 -6.1625130e-02 -6.8644184e-01 -1.4292811e-01\n",
      " -6.8790132e-01  4.3813754e-02 -2.0581808e+00 -1.2410380e+00\n",
      "  2.7950099e-01  2.1052790e+00 -1.3031821e+00 -7.3277718e-01\n",
      " -1.1427141e+00  4.4790220e-01  2.2776430e+00  1.3684369e+00\n",
      "  1.9732231e+00  1.1484480e+00  3.0252426e+00  1.4016727e+00\n",
      " -1.2745086e+00  1.4717708e+00  2.3639221e+00 -2.8872352e+00\n",
      "  1.7449580e+00  6.7229378e-01 -3.4335241e+00  1.9204943e-02\n",
      "  6.9930208e-01 -1.0294513e+00  2.1821392e+00  2.3224537e+00\n",
      " -1.8168439e+00  3.6070745e-02  1.0454433e+00  3.8193295e+00\n",
      " -4.9877599e-01  1.7677529e+00 -1.9869692e+00 -3.7715822e-01\n",
      " -1.5167563e+00  5.7331067e-01  1.0308038e+00  8.1256890e-01\n",
      " -3.6042562e+00  1.9247149e+00 -2.8579840e-03  2.5498924e-01\n",
      "  5.0941592e-01 -1.9462495e+00 -2.3316531e+00 -5.4213798e-01\n",
      " -2.0374866e+00  8.1446058e-01 -2.1217222e+00  5.1972830e-01\n",
      "  1.8602952e+00 -1.9462596e+00  4.1024628e+00 -9.9634236e-01\n",
      "  1.1969291e+00  2.9469488e+00 -1.2441533e+00  9.0021342e-01\n",
      " -1.7375603e+00  2.9401183e+00 -4.5572957e-01 -3.8677545e+00\n",
      " -3.1204033e+00  1.3640295e+00  2.5153253e+00  1.0983527e+00\n",
      " -6.8605393e-01  4.6202769e+00  4.5211210e+00 -3.6785147e+00\n",
      "  1.3308635e-01  4.1584044e+00 -1.7644616e+00 -2.3930804e-01\n",
      " -1.3339965e-01 -2.7817175e-01  2.3597891e+00  3.8709289e-01\n",
      " -1.8116567e+00  1.8674456e+00  1.7927854e+00  1.2760322e+00\n",
      " -3.0657997e+00  1.5326566e+00 -8.3167446e-01  2.9761791e+00\n",
      "  7.0468503e-01  2.8663068e+00 -5.7339716e-01  1.5762058e+00\n",
      "  3.3560276e+00  3.2512684e+00 -2.0908775e-02 -4.5927973e+00\n",
      "  3.7544468e-01 -6.7356533e-01 -1.0314281e+00  1.7492855e-01\n",
      "  1.7940810e+00  3.2585043e-01 -2.8051357e+00 -1.3079505e-01\n",
      "  8.3661014e-01 -2.0233603e-01 -2.0535829e+00 -4.2734319e-01\n",
      "  2.4541907e+00  2.3532245e+00  1.0874225e+00 -1.1641830e+00\n",
      "  2.1354363e+00 -6.4929521e-01  1.8463610e+00 -4.0472336e+00\n",
      " -3.7522011e+00  1.3748828e+00 -3.9129720e+00  4.9684286e-01\n",
      "  2.9526896e+00  1.1606463e-02  3.0158033e+00  2.7554021e+00\n",
      "  2.4392049e+00  9.1806024e-01 -1.4759181e+00 -1.2955189e+00\n",
      "  2.2754374e+00  4.2346520e+00  2.4365509e+00  3.2129960e+00\n",
      " -1.4740580e+00 -7.1382183e-01  2.5501511e+00 -8.5152286e-01\n",
      "  3.6758959e+00  3.6147852e+00  1.0551357e+00 -1.8811873e+00\n",
      " -1.0800929e+00 -2.2777398e+00  1.1658381e+00  2.3186928e-01\n",
      " -2.6699965e+00  5.2132088e-01 -1.7179015e+00  1.4068801e+00\n",
      " -1.8821347e+00 -2.2881513e+00 -2.2091925e+00  1.5779819e+00\n",
      "  8.6430967e-01 -2.5489429e-01  2.3226366e+00  1.8244126e-01\n",
      "  2.9295793e+00 -3.4577242e-01  3.0119267e-01  8.3835876e-01\n",
      "  3.2237232e+00 -6.6842818e-01 -4.8218301e-01  3.2657382e+00\n",
      " -5.9738773e-01  6.3991970e-01  1.9061289e+00  1.2234683e+00\n",
      " -4.8455830e+00  3.5164084e+00  1.0989081e+00 -1.1782404e+00]\n"
     ]
    }
   ],
   "source": [
    "vec = model.wv.get_vector(',')\n",
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 1.0),\n",
       " ('tons', 0.6170707941055298),\n",
       " ('萬零', 0.5534135103225708),\n",
       " ('0k', 0.5509641170501709),\n",
       " ('架次', 0.5040297508239746),\n",
       " ('￥', 0.5037356019020081),\n",
       " ('輛', 0.494095116853714),\n",
       " ('013', 0.4934525787830353),\n",
       " ('AWG', 0.4926324188709259),\n",
       " ('+', 0.49228981137275696)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('婚史', 0.0), ('triangular', 0.0), ('邁杉', 0.0), ('甘尼許', 0.0), ('齊威', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "zero_vec = np.zeros(300)\n",
    "print(model.wv.similar_by_vector(zero_vec,topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#讀取後 加新詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "import logging\n",
    "import sys\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "save_path = r'D:\\Backup\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\sentence\\wiki_word2vec0.model'\n",
    "model = word2vec.Word2Vec.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-19 16:13:41,511 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692776\n",
      "\"word 'woof' not in vocabulary\"\n",
      "\"word '傻眼貓咪' not in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "print(len(model.wv.index2word))\n",
    "try:\n",
    "    model.wv.most_similar(positive='woof',topn=5)\n",
    "except KeyError as err:\n",
    "    print(err)\n",
    "try:\n",
    "    model.wv.most_similar(positive='傻眼貓咪',topn=5)\n",
    "except KeyError as err:\n",
    "    print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('糾紛案件', 0.2756342887878418),\n",
       " ('根本原因', 0.27125775814056396),\n",
       " ('離散性', 0.2641603648662567),\n",
       " ('金融市場', 0.2583328187465668),\n",
       " ('隨機變量', 0.25302454829216003)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#版本1\n",
    "model.vocabulary.min_count = 1\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model.build_vocab(sentences, update=True)#训练该行\n",
    "model.train(sentences,total_examples= model.corpus_count,epochs= model.epochs)\n",
    "print(len(model.wv.index2word))\n",
    "model.wv.most_similar(positive='woof',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Zeonic', 0.2712220251560211),\n",
       " ('自衛權', 0.25267353653907776),\n",
       " ('通比', 0.23276172578334808),\n",
       " ('第一次', 0.2252526432275772),\n",
       " ('利載', 0.21481187641620636)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#版本2\n",
    "model.vocabulary.min_count = 1   #修改之前訓練時的設定(新增的句子較少所以 詞的最小出現次數改為1)\n",
    "file_path = r'D:\\Backup\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\sentence\\test.txt'\n",
    "sentences = word2vec.LineSentence(file_path)\n",
    "model.build_vocab(sentences, update=True)#训练该行\n",
    "newmodel = model.train(sentences,total_examples= model.corpus_count,epochs= model.epochs)\n",
    "print(len(model.wv.index2word))\n",
    "model.wv.most_similar(positive='傻眼貓咪',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('輻射區', 0.259126216173172),\n",
       " ('林耕仁', 0.2490512579679489),\n",
       " ('間', 0.24377873539924622),\n",
       " ('佔領區', 0.2437250018119812),\n",
       " ('34.83', 0.24121515452861786)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#版本3\n",
    "file_path = r'D:\\Backup\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\sentence\\test.txt'\n",
    "model.vocabulary.min_count = 1   #修改之前訓練時的設定(新增的句子較少所以 詞的最小出現次數改為1)\n",
    "with open(file_path,'r',encoding='utf-8') as f:\n",
    "    sentence = []\n",
    "    for line in f:\n",
    "        sentence.append(line.strip('\\n').strip(' ').split(' '))\n",
    "    model.build_vocab(sentence, update=True)#训练该行\n",
    "    model.train(sentence,total_examples= model.corpus_count,epochs= model.epochs)\n",
    "print(len(model.wv.index2word))\n",
    "model.wv.most_similar(positive='傻眼貓咪',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#文件訓練word2vec，儲存\n",
    "def Word2vec_train(file_path,save_path,dir_path=None,save_name='word2vec_model',replace_old=False,\n",
    "                   model_size=300,model_window=10,model_min_count=5,**kw):\n",
    "    \"\"\"\n",
    "    batch train usage: set dir_path、save_name, file_path = None, save_path = None\n",
    "    if Multiple files using dir_path\n",
    "    \"\"\"\n",
    "    from gensim.models import word2vec\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    if file_path != None:\n",
    "        #單檔案\n",
    "        sentences = word2vec.LineSentence(file_path)\n",
    "        model = word2vec.Word2Vec(sentences, size=model_size,window=model_window,min_count=model_min_count,**kw)\n",
    "        #保存模型，供日後使用\n",
    "        model.save(save_path)\n",
    "    if dir_path != None and file_path == None:\n",
    "        #多檔案\n",
    "        sentences = word2vec.PathLineSentences(dir_path)\n",
    "        model = word2vec.Word2Vec(sentences, size=model_size,window=model_window,min_count=model_min_count,**kw)\n",
    "        #保存模型，供日後使用\n",
    "        model.save(os.path.join(dir_path,save_name))\n",
    "    #模型讀取方式\n",
    "    # model = word2vec.Word2Vec.load(\"your_model_name\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def main(file_path,save_path):\n",
    "\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    sentences = word2vec.LineSentence(file_path)\n",
    "    #多檔案\n",
    "#     sentences = word2vec.PathLineSentences(segment_dir)\n",
    "    model = word2vec.Word2Vec(sentences, size=280,window=10,min_count=5)\n",
    "\n",
    "    #保存模型，供日後使用\n",
    "    model.save(save_path)\n",
    "\n",
    "    #模型讀取方式\n",
    "    # model = word2vec.Word2Vec.load(\"your_model_name\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = r'D:\\ml_data\\data_set\\NLP\\wiki_seg.txt'\n",
    "    save_path = r'D:\\ml_data\\data_set\\NLP\\wiki_word2vec3.model'\n",
    "    main(file_path,save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from opencc import OpenCC\n",
    "import re\n",
    "import logging\n",
    "import jieba\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "save_path = r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\sentence\\wiki_word2vec0.model'\n",
    "word_vec_model = word2vec.Word2Vec.load(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = r'D:\\ml_data\\Data_set\\NLP\\chinese_sentence_collection\\Tidied_Corpus\\chat'\n",
    "file_name = r'merge_chat.txt'\n",
    "file_path = os.path.join(dir_path,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(file_path,max_word_num):\n",
    "    with open(file_path,'r',encoding='utf-8') as f:\n",
    "        new_list = []\n",
    "        for chat in f:\n",
    "            #依行讀取\n",
    "            save_chat = chat.strip('\\n').split('\\t')\n",
    "            X = save_chat[0].split(' ')\n",
    "            Y = save_chat[1].split(' ')\n",
    "            #如果詞數大於30\n",
    "            if len(X) < max_word_num and len(Y) < max_word_num:\n",
    "                for num in range(max_word_num-len(X)):\n",
    "                    X.append(0)\n",
    "                for num in range(max_word_num-len(Y)):\n",
    "                    Y.append(0)\n",
    "                new_list.append([X,Y])\n",
    "            if len(save_chat) < 2:\n",
    "                print(save_chat)\n",
    "                break\n",
    "        return new_list\n",
    "chat_data = trim(file_path,max_word_num=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94319"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "def word_to_vec(index):\n",
    "    padding = np.zeros((1,300),dtype=np.float32)\n",
    "    chat = chat_data[index]\n",
    "    #X\n",
    "    vec_array = np.zeros((1,300),dtype=np.float32)\n",
    "    for word in chat[0]:\n",
    "        if word == 0:\n",
    "            vec = padding\n",
    "            vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "        else:\n",
    "            try:\n",
    "                vec = np.array(word_vec_model.wv.get_vector(word),np.float32).reshape(1,300)\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "            except KeyError:\n",
    "                vec = padding\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "    X = vec_array[1:]\n",
    "    #Y\n",
    "    vec_array = np.zeros((1,300),dtype=np.float32)\n",
    "    for word in chat[1]:\n",
    "        if word == 0:\n",
    "            vec = padding\n",
    "            vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "        else:\n",
    "            try:\n",
    "                vec = np.array(word_vec_model.wv.get_vector(word),np.float32).reshape(1,300)\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "            except KeyError:\n",
    "                vec = padding\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "    Y = vec_array[1:]\n",
    "    return X,Y\n",
    "\n",
    "X,Y = word_to_vec(0)\n",
    "len(chat_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 20, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "padding = np.zeros((1,300),dtype=np.float32)\n",
    "X = []\n",
    "Y = []\n",
    "for chat in chat_data[:1]:\n",
    "    vec_array = np.zeros((1,300),dtype=np.float32)\n",
    "    for word in chat[0]:\n",
    "        if word == 0:\n",
    "            vec = padding\n",
    "            vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "        else:\n",
    "            try:\n",
    "                vec = np.array(word_vec_model.wv.get_vector(word),np.float32).reshape(1,300)\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "            except KeyError:\n",
    "                vec = zero_vec\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "    X.append(vec_array[1:])\n",
    "    #-----#\n",
    "    vec_array = np.zeros((1,300),dtype=np.float32)\n",
    "    for word in chat[1]:\n",
    "        if word == 0:\n",
    "            vec = padding\n",
    "            vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "        else:\n",
    "            try:\n",
    "                vec = np.array(word_vec_model.wv.get_vector(word),np.float32).reshape(1,300)\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "            except KeyError:\n",
    "                vec = zero_vec\n",
    "                vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "    Y.append(vec_array[1:])\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
