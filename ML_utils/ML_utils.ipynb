{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "opencc: https://github.com/yichen0831/opencc-python\n",
    "batch using dir_file_call_function()\n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import sys \n",
    "import shutil\n",
    "import logging\n",
    "\n",
    "#多圖片繪圖(batch,width,height,c)\n",
    "def draw_im_poly(image,im_num):\n",
    "    import matplotlib.pyplot as plt\n",
    "    row_num = int(im_num/5) + 1\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    for num in range(im_num):        \n",
    "        subfig = plt.subplot(row_num,5,num+1)\n",
    "        subfig.axis('off')\n",
    "        subfig.imshow(image[num],cmap='gray')\n",
    "    plt.show()\n",
    "\n",
    "#取得文件資料列表(包括資料夾)\n",
    "def Get_dir_file_list(dir_path,file_filter_=None,distinguish=False,regular=False):\n",
    "    \"\"\"\n",
    "    filter: Filter files whose file names do not include strings\n",
    "    distinguish: Distinguish between folders and file,True=Distinguish\n",
    "    \"\"\"\n",
    "    dir_path = dir_path\n",
    "    file_list = os.listdir(dir_path)\n",
    "    file_list_ = []\n",
    "    dir_list_ = []\n",
    "    if distinguish == False:\n",
    "        if file_filter_ == None:\n",
    "            for file in file_list:\n",
    "                file_list_.append(file)\n",
    "        else:\n",
    "            if regular == False:\n",
    "                for file in file_list:\n",
    "                    if file_filter_ in file:\n",
    "                        file_list_.append(file)\n",
    "            if regular == True:\n",
    "                for file in file_list:\n",
    "                    if re.search(file_filter_, file) != None:\n",
    "                        file_list_.append(file)       \n",
    "        file_list = file_list_\n",
    "    if distinguish == True:\n",
    "        if file_filter_ == None:\n",
    "            for file in file_list:\n",
    "                if os.path.isfile( os.path.join(dir_path,file) ) == True:\n",
    "                    file_list_.append(file)\n",
    "                elif os.path.isdir( os.path.join(dir_path,file) ) == True:\n",
    "                    dir_list_.append(file)\n",
    "        else:\n",
    "            if regular == False:\n",
    "                for file in file_list:\n",
    "                    if file_filter_ in file:\n",
    "                        if os.path.isfile( os.path.join(dir_path,file) ) == True:\n",
    "                            file_list_.append(file)\n",
    "                        elif os.path.isdir( os.path.join(dir_path,file) ) == True:\n",
    "                            dir_list_.append(file)\n",
    "            if regular == True:\n",
    "                for file in file_list:\n",
    "                    if re.search(file_filter_, file) != None:\n",
    "                        if os.path.isfile( os.path.join(dir_path,file) ) == True:\n",
    "                            file_list_.append(file)\n",
    "                        elif os.path.isdir( os.path.join(dir_path,file) ) == True:\n",
    "                            dir_list_.append(file)\n",
    "        file_list = [file_list_,dir_list_]\n",
    "    return file_list\n",
    "\n",
    "#資料夾內檔案由function處理完後儲存\n",
    "def CallF_DirFile_save(dir_path,function,file_head_name='new_',replace_old=False,file_filter_=None,regular=False,**kw):\n",
    "    \"\"\"\n",
    "    **kw ==>> function(file_path=file_path,save_path=save_path,**kw)    ex. conversion='s2t'\n",
    "    expansion: \n",
    "    def function(file_path,save_path,replace_old,....):\n",
    "        .....\n",
    "        if replace_old == True:\n",
    "        shutil.move(save_path,file_path)   \n",
    "    \"\"\"\n",
    "    file_list = Get_dir_file_list(dir_path=dir_path,file_filter_=file_filter_,regular=regular,distinguish=True)\n",
    "    file_list = file_list[0]\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(dir_path,file_name)\n",
    "        save_path = os.path.join(dir_path,file_head_name+file_name)\n",
    "        function(file_path=file_path,save_path=save_path,replace_old=replace_old,**kw)\n",
    "\n",
    "#資料夾內檔案由function處理完後存成list返回\n",
    "def CallF_DirFile(dir_path,function,file_filter_=None,regular=False,**kw):\n",
    "    \"\"\"\n",
    "    return list\n",
    "    **kw ==>> function(file_path=file_path,save_path=save_path,**kw)    ex. conversion='s2t'\n",
    "    expansion: def function(file_path,save_path,replace_old,....)\n",
    "    \"\"\"\n",
    "    file_list = Get_dir_file_list(dir_path=dir_path,file_filter_=file_filter_,regular=regular,distinguish=True)\n",
    "    file_list = file_list[0]\n",
    "    save_list = []\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(dir_path,file_name)\n",
    "        save_list.append(function(file_path=file_path,**kw))\n",
    "    return save_list\n",
    "\n",
    "#合併資料夾內文件，儲存\n",
    "def Merge_dir_file(dir_path,save_name='dir_file_merge',file_filter_=None,regular=False,\n",
    "                   add_line_Feed=True,file_remove_LF=False,encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    ::parameter::\n",
    "    filter: Filter files whose file names do not include strings\n",
    "    file_remove_LF: read file and  remove LF\n",
    "    add_line_Feed: add LF after file merge    \n",
    "    \"\"\"\n",
    "    file_list = Get_dir_file_list(dir_path,file_filter_=file_filter_,regular=regular)\n",
    "    file_merge = \"\"\n",
    "    save_path = os.path.join(dir_path,save_name)\n",
    "    for file_name in file_list: \n",
    "        file_path = os.path.join(dir_path,file_name)\n",
    "        with open(file_path,'r',encoding=encoding) as f:\n",
    "            read_file = f.read()\n",
    "            if file_remove_LF == True:\n",
    "                read_file = Remove_str_LR(read_file)\n",
    "            file_merge += read_file\n",
    "            if add_line_Feed == True:\n",
    "                file_merge = file_merge + '\\n'\n",
    "    with open(save_path,'w',encoding=encoding) as save:\n",
    "        save.write(file_merge)\n",
    "    return file_merge\n",
    "def Remove_str_LR(str_in):\n",
    "    str_out = re.sub(r\"\\n\", r\"\", str_in)\n",
    "    return str_out\n",
    "\n",
    "def Shuffle_file(file_path,save_path,replace_old=False,encoding='utf-8'):\n",
    "    import random\n",
    "    with open(file_path, 'r',encoding=encoding) as f:\n",
    "        f_list = list(f)\n",
    "        random.shuffle(f_list)\n",
    "    with open(save_path, 'w',encoding=encoding) as f:\n",
    "        for line in f_list:\n",
    "            f.write(line)\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path,file_path)\n",
    "\n",
    "#移除文件中重複的row，儲存\n",
    "def Remove_file_repeat_row(file_path,save_path,replace_old=False,encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    batch using dir_file_call_function()\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r',encoding=encoding) as f:\n",
    "        out = ''.join(list(set([i for i in f])))\n",
    "    with open(save_path, 'w',encoding=encoding) as f:\n",
    "        f.write(out)\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path,file_path)\n",
    "\n",
    "#過濾文件中長度大於max_word_num的row，儲存\n",
    "def Filter_file_wlen(file_path,save_path,max_word_num,replace_old=False,encoding='utf-8',\n",
    "                     word_split=' ',row_split='\\n',padding=None):\n",
    "    \"\"\"\n",
    "    Filter out longer of line than max_word_num\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding=encoding) as f:\n",
    "        with open(save_path, 'w',encoding=encoding) as f_wrtie:\n",
    "            if padding == None:\n",
    "                for line in f:\n",
    "                    line_ = line.strip('\\n')\n",
    "                    word_list = line_.split(word_split)\n",
    "                    if len(word_list) <= max_word_num:\n",
    "                        line_ = line_+ row_split\n",
    "                        f_wrtie.write(line_)\n",
    "            if padding != None:\n",
    "                for line in f:\n",
    "                    padding_seq = ''\n",
    "                    line_ = line.strip('\\n').strip()\n",
    "                    word_list = line_.split(word_split)\n",
    "                    if len(word_list) <= max_word_num:\n",
    "                        for num in range(max_word_num-len(word_list)):\n",
    "                            word_list.append(padding)\n",
    "                        for seq in word_list:\n",
    "                            padding_seq = padding_seq+ seq + word_split\n",
    "                        padding_seq = padding_seq + row_split\n",
    "                        f_wrtie.write(padding_seq)\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path,file_path)\n",
    "\n",
    "#填補文件row中字數小於max_word_num，儲存\n",
    "def Padding_file_lword(file_path,save_path,max_word_num,replace_old=False,encoding='utf-8',\n",
    "                       padding='my_padding_str',word_split=' ',row_split='\\n'):\n",
    "    \"\"\"\n",
    "    The number of words in the line does not exceed max_word_num, run padding.\n",
    "    \"\"\"\n",
    "    with open(file_path,'r',encoding=encoding) as f:\n",
    "        with open(save_path, 'w',encoding=encoding) as f_wrtie:\n",
    "            for line in f:\n",
    "                #依行讀取\n",
    "                padding_seq = ''\n",
    "                line_ = line.strip('\\n').strip()\n",
    "                word_list = line_.split(word_split)\n",
    "                if len(word_list) <= max_word_num:\n",
    "                    for num in range(max_word_num-len(word_list)):\n",
    "                        word_list.append(padding)\n",
    "                    for seq in word_list:\n",
    "                        padding_seq = padding_seq+ seq + word_split\n",
    "                    padding_seq = padding_seq + row_split\n",
    "                    f_wrtie.write(padding_seq)\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path,file_path)    \n",
    "\n",
    "#裁切文件row數\n",
    "def Trim_file_rows(file_path,save_path,row_num=1000,n_times=False,replace_old=False,encoding='utf-8',sep='\\n'):\n",
    "    import pandas as pd\n",
    "    num = 1\n",
    "    data = pd.read_csv(file_path,encoding=encoding,sep=sep,header=-1)\n",
    "    if n_times != False:\n",
    "        num = len(data)//row_num\n",
    "    data[:num*row_num].to_csv(save_path,sep='\\n',header=False,index=False)\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path,file_path)\n",
    "\n",
    "#分詞str返回str\n",
    "def Jieba_str_segmentation(string,delimiter=' ',stopword_path=None,split=False,encoding='utf-8',load_userdict_path=None):\n",
    "    \"\"\"\n",
    "    split: string to list\n",
    "    stopword_path: delimiter of stopword must is '\\n'\n",
    "    \"\"\"\n",
    "    import jieba\n",
    "    # jieba custom setting.\n",
    "    jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "    #加新詞\n",
    "    if load_userdict_path != None:\n",
    "        jieba.load_userdict(load_userdict_path)\n",
    "    # load stopwords set\n",
    "    #將停用詞每row分別加進集合\n",
    "    stopword_set = set()\n",
    "    if stopword_path != None:\n",
    "        #設置停用詞讀取路徑\n",
    "        with open(stopword_path,'r', encoding=encoding) as stopwords:\n",
    "            for stopword in stopwords:\n",
    "                stopword_set.add(stopword.strip('\\n'))   #移除頭尾換行 strip('\\n')\n",
    "    output = ''\n",
    "    string = string.strip('\\n')\n",
    "    words = jieba.cut(string, cut_all=False,HMM=True)    #進行斷詞\n",
    "    for word in words:\n",
    "        #依每個詞判斷是否為停用詞(不是就寫入)\n",
    "        if word not in stopword_set:\n",
    "            output = output+word+delimiter\n",
    "    if split == True:\n",
    "        output = output.split(delimiter)\n",
    "    return output\n",
    "\n",
    "#轉換str返回str\n",
    "def Opencc_str(string,conversion='s2t'):\n",
    "    \"\"\"\n",
    "    opencc:\n",
    "    https://github.com/yichen0831/opencc-python\n",
    "    conversion: \n",
    "    hk2s: Traditional Chinese (Hong Kong standard) to Simplified Chinese\n",
    "    s2hk: Simplified Chinese to Traditional Chinese (Hong Kong standard)\n",
    "    s2t: Simplified Chinese to Traditional Chinese\n",
    "    s2tw: Simplified Chinese to Traditional Chinese (Taiwan standard)\n",
    "    s2twp: Simplified Chinese to Traditional Chinese (Taiwan standard, with phrases)\n",
    "    t2hk: Traditional Chinese to Traditional Chinese (Hong Kong standard)\n",
    "    t2s: Traditional Chinese to Simplified Chinese\n",
    "    t2tw: Traditional Chinese to Traditional Chinese (Taiwan standard)\n",
    "    tw2s: Traditional Chinese (Taiwan standard) to Simplified Chinese\n",
    "    tw2sp: Traditional Chinese (Taiwan standard) to Simplified Chinese (with phrases)\n",
    "    \"\"\"\n",
    "    from opencc import OpenCC\n",
    "    cc = OpenCC(conversion)\n",
    "    out =  cc.convert(string)\n",
    "    return out\n",
    "\n",
    "#分詞文件，儲存\n",
    "def Jieba_file_segmentation(file_path,save_path,replace_old=False,word_delimiter=' ',\n",
    "                            file_delimiter='\\n',stopword_path=None,encoding='utf-8',load_userdict_path=None):\n",
    "    \"\"\"\n",
    "    dictionary_path: \".jieba_dict/dict.txt.big\"\n",
    "    batch using dir_file_call_function()\n",
    "    close log using \"logging.disable(lvl)\"\n",
    "    https://docs.python.org/3/library/logging.html\n",
    "    stopword_path: delimiter of stopword must is '\\n'\n",
    "    \"\"\"\n",
    "    import jieba\n",
    "    #設置log格式，以及print的log等級\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # jieba custom setting.\n",
    "    jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "    #加新詞\n",
    "    if load_userdict_path != None:\n",
    "        jieba.load_userdict(load_userdict_path)\n",
    "    #將停用詞每row分別加進集合\n",
    "    stopword_set = set()\n",
    "    if stopword_path != None:\n",
    "        with open(stopword_path,'r', encoding=encoding) as stopwords:\n",
    "            for stopword in stopwords:\n",
    "                stopword_set.add(stopword.strip('\\n'))   #移除頭尾換行 strip('\\n')\n",
    "    #open write file\n",
    "    output = open(save_path, 'w', encoding=encoding)\n",
    "    with open(file_path, 'r', encoding=encoding) as content :\n",
    "        #每一行都切成一個iter\n",
    "        for texts_num, line in enumerate(content):\n",
    "            line = line.strip('\\n')\n",
    "            words = jieba.cut(line, cut_all=False,HMM=True)    #進行斷詞\n",
    "            for word in words:\n",
    "                #依每個詞判斷是否為停用詞(不是就寫入)\n",
    "                if word not in stopword_set:\n",
    "                    output.write(word + word_delimiter)     #每一行的iter(詞)以空格隔開\n",
    "            output.write(file_delimiter)      #iter完以換行符區隔\n",
    "            if (texts_num + 1) % 10000 == 0:\n",
    "                logging.info(\"已完成前 %d 行的斷詞\" % (texts_num + 1))\n",
    "    output.close()\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path,file_path)\n",
    "\n",
    "#轉換文件，儲存\n",
    "def Opencc_file(file_path,save_path,replace_old=False,conversion='s2t',encoding='utf-8'):\n",
    "    \"\"\"\n",
    "    batch using dir_file_call_function()\n",
    "    close log using \"logging.disable(lvl)\"\n",
    "    https://docs.python.org/3/library/logging.html\n",
    "    opencc:\n",
    "    https://github.com/yichen0831/opencc-python\n",
    "    conversion: \n",
    "    hk2s: Traditional Chinese (Hong Kong standard) to Simplified Chinese\n",
    "    s2hk: Simplified Chinese to Traditional Chinese (Hong Kong standard)\n",
    "    s2t: Simplified Chinese to Traditional Chinese\n",
    "    s2tw: Simplified Chinese to Traditional Chinese (Taiwan standard)\n",
    "    s2twp: Simplified Chinese to Traditional Chinese (Taiwan standard, with phrases)\n",
    "    t2hk: Traditional Chinese to Traditional Chinese (Hong Kong standard)\n",
    "    t2s: Traditional Chinese to Simplified Chinese\n",
    "    t2tw: Traditional Chinese to Traditional Chinese (Taiwan standard)\n",
    "    tw2s: Traditional Chinese (Taiwan standard) to Simplified Chinese\n",
    "    tw2sp: Traditional Chinese (Taiwan standard) to Simplified Chinese (with phrases)\n",
    "    \"\"\"\n",
    "    from opencc import OpenCC\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    cc = OpenCC(conversion)\n",
    "    string = ''\n",
    "    with open(file_path,'r',encoding=encoding) as read_f:\n",
    "        with open(save_path,'w',encoding=encoding) as write_f:\n",
    "            for texts_num,read_line in enumerate(read_f):\n",
    "                file_str =  cc.convert(read_line)\n",
    "                write_f.writelines(file_str)\n",
    "                if (texts_num + 1) % 10000 == 0:\n",
    "                    logging.info(\"已完成前 %d 行的轉換\" % (texts_num + 1))\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path,file_path)\n",
    "\n",
    "#文件訓練word2vec，儲存\n",
    "def Word2vec_train(file_path,save_path,dir_path=None,save_name='word2vec_model',replace_old=False,\n",
    "                   model_size=300,model_window=10,model_min_count=5,**kw):\n",
    "    \"\"\"\n",
    "    batch train usage: set dir_path、save_name, file_path = None, save_path = None\n",
    "    if Multiple files using dir_path\n",
    "    \"\"\"\n",
    "    #\n",
    "    from gensim.models import word2vec\n",
    "    logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "    # https://radimrehurek.com/gensim/models/word2vec.html\n",
    "    if file_path != None:\n",
    "        #單檔案\n",
    "        sentences = word2vec.LineSentence(file_path)\n",
    "        model = word2vec.Word2Vec(sentences, size=model_size,window=model_window,min_count=model_min_count,**kw)\n",
    "        #保存模型，供日後使用\n",
    "        model.save(save_path)\n",
    "    if dir_path != None and file_path == None:\n",
    "        #多檔案\n",
    "        sentences = word2vec.PathLineSentences(dir_path)\n",
    "        model = word2vec.Word2Vec(sentences, size=model_size,window=model_window,min_count=model_min_count,**kw)\n",
    "        #保存模型，供日後使用\n",
    "        model.save(os.path.join(dir_path,save_name))\n",
    "    #模型讀取方式\n",
    "    # model = word2vec.Word2Vec.load(\"your_model_name\") \n",
    "\n",
    "#文件的字轉成list，返回list\n",
    "def WordToList_file(file_path,encoding='utf-8',max_word_num=None,word_split=' ',row_split='\\n',padding=None):\n",
    "    with open(file_path,'r',encoding=encoding) as f:\n",
    "        save_list = []\n",
    "        if max_word_num == None:\n",
    "            for line in f:\n",
    "                line_ = line.strip('\\n').strip()\n",
    "                word_list = line_.split(word_split)\n",
    "                save_list.append(word_list)\n",
    "        if max_word_num != None:\n",
    "            if padding == None:\n",
    "                for line in f:\n",
    "                    line_ = line.strip('\\n').strip()\n",
    "                    word_list = line_.split(word_split)\n",
    "                    if len(word_list) <= max_word_num:\n",
    "                        save_list.append(word_list)\n",
    "            if padding != None:\n",
    "                for line in f:\n",
    "                    padding_seq = ''\n",
    "                    line_ = line.strip('\\n').strip()\n",
    "                    word_list = line_.split(word_split)\n",
    "                    if len(word_list) <= max_word_num:\n",
    "                        for num in range(max_word_num-len(word_list)):\n",
    "                            word_list.append(padding)\n",
    "                        save_list.append(word_list)\n",
    "    return save_list\n",
    "    \n",
    "#文件依word2vec模型轉vec後存成npy\n",
    "def ToVec_file_save(file_path,save_path,vec_model,vec_padding,\n",
    "                replace_old=False,encoding='utf-8',word_padding=None,word_padding_vec=None,dim=300):\n",
    "    \"\"\"\n",
    "    vec_model : Word2vec model\n",
    "    dim = Dimension of Word2vec\n",
    "    vec_padding: Words that are not found in 'word2vec' are filled.\n",
    "    word_padding: Padded words in the line of original file\n",
    "    word_padding_vec: vec of word_padding\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    with open(file_path,'r',encoding=encoding) as f:\n",
    "        X = []\n",
    "        for line in f:\n",
    "            vec_array = np.zeros((1,dim),dtype=np.float32)\n",
    "            line_ = line.strip('\\n').strip().split(' ')\n",
    "            for word in line_:\n",
    "                if word == word_padding and type(word_padding_vec) != None:\n",
    "                    vec_array = np.concatenate((vec_array,word_padding_vec),axis=0)\n",
    "                else:\n",
    "                    try:\n",
    "                        vec = np.array(vec_model.wv.get_vector(word),np.float32).reshape(1,dim)\n",
    "                        vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "                    except KeyError:\n",
    "                        vec_array = np.concatenate((vec_array,vec_padding),axis=0)\n",
    "            X.append(vec_array[1:])\n",
    "    np.save(save_path,X)\n",
    "    if replace_old == True:\n",
    "        shutil.move(save_path+'.npy',file_path)\n",
    "\n",
    "#文件依word2vec模型轉vec後返回ndarray\n",
    "def ToVec_file(file_path,vec_model,vec_padding,\n",
    "               encoding='utf-8',word_padding=None,word_padding_vec=None,dim=300):\n",
    "    \"\"\"\n",
    "    vec_model : Word2vec model\n",
    "    dim = Dimension of Word2vec\n",
    "    vec_padding: Words that are not found in 'word2vec' are filled.\n",
    "    word_padding: Padded words in the line of original file\n",
    "    word_padding_vec: vec of word_padding\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    with open(file_path,'r',encoding=encoding) as f:\n",
    "        X = []\n",
    "        for line in f:\n",
    "            vec_array = np.zeros((1,dim),dtype=np.float32)\n",
    "            line_ = line.strip('\\n').strip().split(' ')\n",
    "            for word in line_:\n",
    "                if word == word_padding and type(word_padding_vec) != None:\n",
    "                    vec_array = np.concatenate((vec_array,word_padding_vec),axis=0)\n",
    "                else:\n",
    "                    try:\n",
    "                        vec = np.array(vec_model.wv.get_vector(word),np.float32).reshape(1,dim)\n",
    "                        vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "                    except KeyError:\n",
    "                        vec_array = np.concatenate((vec_array,vec_padding),axis=0)\n",
    "            X.append(vec_array[1:])\n",
    "        X = np.array(X)\n",
    "    return X\n",
    "\n",
    "#list依word2vec模型轉vec後返回ndarray\n",
    "def ToVec_list(line_list,vec_model,vec_padding,word_padding=None,word_padding_vec=None,dim=300):\n",
    "    \"\"\"\n",
    "    vec_model : Word2vec model\n",
    "    dim = Dimension of Word2vec\n",
    "    vec_padding: Words that are not found in 'word2vec' are filled.\n",
    "    word_padding: Padded words in the line of original file\n",
    "    word_padding_vec: vec of word_padding\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    X = []\n",
    "    for line in line_list:\n",
    "        vec_array = np.zeros((1,dim),dtype=np.float32)\n",
    "        for word in line:\n",
    "            if word == word_padding and type(word_padding_vec) != None:\n",
    "                vec_array = np.concatenate((vec_array,word_padding_vec),axis=0)\n",
    "            else:\n",
    "                try:\n",
    "                    vec = np.array(vec_model.wv.get_vector(word),np.float32).reshape(1,dim)\n",
    "                    vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "                except KeyError:\n",
    "                    vec_array = np.concatenate((vec_array,vec_padding),axis=0)\n",
    "        X.append(vec_array[1:])\n",
    "    X = np.array(X)\n",
    "    return X\n",
    "\n",
    "#list依word2vec模型轉vec後儲存\n",
    "def ToVec_list_save(line_list,save_path,vec_model,vec_padding,word_padding=None,word_padding_vec=None,dim=300):\n",
    "    \"\"\"\n",
    "    vec_model : Word2vec model\n",
    "    dim = Dimension of Word2vec\n",
    "    vec_padding: Words that are not found in 'word2vec' are filled.\n",
    "    word_padding: Padded words in the line of original file\n",
    "    word_padding_vec: vec of word_padding\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    X = []\n",
    "    for line in line_list:\n",
    "        vec_array = np.zeros((1,dim),dtype=np.float32)\n",
    "        for word in line:\n",
    "            if word == word_padding and type(word_padding_vec) != None:\n",
    "                vec_array = np.concatenate((vec_array,word_padding_vec),axis=0)\n",
    "            else:\n",
    "                try:\n",
    "                    vec = np.array(vec_model.wv.get_vector(word),np.float32).reshape(1,dim)\n",
    "                    vec_array = np.concatenate((vec_array,vec),axis=0)\n",
    "                except KeyError:\n",
    "                    vec_array = np.concatenate((vec_array,vec_padding),axis=0)\n",
    "        X.append(vec_array[1:])\n",
    "    X = np.array(X)\n",
    "    np.save(save_path,X)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
